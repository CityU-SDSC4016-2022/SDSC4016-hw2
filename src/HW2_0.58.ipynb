{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Import necessary packages.\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, SubsetRandomSampler\nfrom torchvision.datasets import DatasetFolder\nfrom sklearn.model_selection import KFold\nfrom matplotlib import pyplot as plt\nfrom scipy.signal import savgol_filter\n\n#If you haven't download the tqdm package, just uncomment the following line.\n#!pip install tqdm\n# This is for the progress bar.\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-09T03:51:40.862715Z","iopub.execute_input":"2023-03-09T03:51:40.863406Z","iopub.status.idle":"2023-03-09T03:51:45.301685Z","shell.execute_reply.started":"2023-03-09T03:51:40.863359Z","shell.execute_reply":"2023-03-09T03:51:45.299904Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# It is important to do data augmentation in training.\n# However, not every augmentation is useful.\n# Please think about what kind of augmentation is helpful for food recognition.\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((128, 128)),\n    \n    # Randomly rotate the image by up to 10 degrees\n    transforms.RandomRotation(degrees=90),\n    # Randomly zoom in or out on the image\n    transforms.RandomResizedCrop(size=128, scale=(0.7, 1.3)),\n    # Randomly crop a portion of the image\n    transforms.RandomCrop(size=128, padding=10),\n    # Randomly adjust the brightness and contrast of the image\n    transforms.ColorJitter(brightness=0.5, contrast=0.5),\n    # Convert the image to a tensor\n    \n    transforms.ToTensor(),\n])\n\n\n# We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# Batch size for training, validation, and testing.\n# A greater batch size usually gives a more stable gradient.\n# But the GPU memory is limited, so please adjust it carefully.\nbatch_size = 400\n\n# Construct datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntrain_set = DatasetFolder(\"/kaggle/input/sdsc4016-fundls-of-ml-2-hw2/food-11/food-11/training/labeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\nvalid_set = DatasetFolder(\"/kaggle/input/sdsc4016-fundls-of-ml-2-hw2/food-11/food-11/validation\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\nunlabeled_set = DatasetFolder(\"/kaggle/input/sdsc4016-fundls-of-ml-2-hw2/food-11/food-11/training/unlabeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\ntest_set = DatasetFolder(\"/kaggle/input/sdsc4016-fundls-of-ml-2-hw2/food-11/food-11/testing\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n\n# Construct data loaders.\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-03-09T03:51:45.304320Z","iopub.execute_input":"2023-03-09T03:51:45.305026Z","iopub.status.idle":"2023-03-09T03:51:56.179149Z","shell.execute_reply.started":"2023-03-09T03:51:45.304976Z","shell.execute_reply":"2023-03-09T03:51:56.177889Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # The arguments for commonly used modules:\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n\n        # input image size: [3, 128, 128]\n        self.cnn_layers = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),\n\n            nn.Conv2d(64, 128, 3, 1, 1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),\n\n            nn.Conv2d(128, 256, 3, 1, 1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(4, 4, 0),\n            \n            nn.Conv2d(256, 512, 3, 1, 1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(4, 4, 0),\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Linear(512 * 2 * 2, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 11)\n        )\n\n    def forward(self, x):\n        # input (x): [batch_size, 3, 128, 128]\n        # output: [batch_size, 11]\n\n        # Extract features by convolutional layers.\n        x = self.cnn_layers(x)\n\n        # The extracted feature map must be flatten before going to fully-connected layers.\n        x = x.flatten(1)\n\n        # The features are transformed by fully-connected layers to obtain the final logits.\n        x = self.fc_layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-03-09T03:51:56.180650Z","iopub.execute_input":"2023-03-09T03:51:56.181084Z","iopub.status.idle":"2023-03-09T03:51:56.193641Z","shell.execute_reply.started":"2023-03-09T03:51:56.181037Z","shell.execute_reply":"2023-03-09T03:51:56.192200Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def get_pseudo_labels(dataset, model, threshold=0.65):\n    # This functions generates pseudo-labels of a dataset using given model.\n    # It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.\n    # You are NOT allowed to use any models trained on external data for pseudo-labeling.\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Make sure the model is in eval mode.\n    model.eval()\n    # Define softmax function.\n    softmax = nn.Softmax(dim=-1)\n    \n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    # Iterate over the dataset by batches.\n    for batch in tqdm(dataloader):\n        img, _ = batch\n\n        # Forward the data\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = model(img.to(device))\n\n        # Obtain the probability distributions by applying softmax on logits.\n        probs = softmax(logits)\n\n        # you may filter the data and construct a new dataset here.\n\n    # # Turn off the eval mode.\n    model.train()\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-09T03:51:56.199624Z","iopub.execute_input":"2023-03-09T03:51:56.200993Z","iopub.status.idle":"2023-03-09T03:51:56.230030Z","shell.execute_reply.started":"2023-03-09T03:51:56.200937Z","shell.execute_reply":"2023-03-09T03:51:56.228969Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def Training( model, device, criterion, optimizer, n_epochs, do_semi, train_loader,test_loader,unlabeled_set):\n    history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[]}\n    for epoch in range(n_epochs):\n        # ---------- TODO ----------\n        # In each epoch, relabel the unlabeled dataset for semi-supervised learning.\n        # Then you can combine the labeled dataset and pseudo-labeled dataset for the training.\n        if do_semi:\n            # Obtain pseudo-labels for unlabeled data using trained model.\n            pseudo_set = get_pseudo_labels(unlabeled_set, model)\n\n            # Construct a new dataset and a data loader for training.\n            # This is used in semi-supervised learning only.\n            concat_dataset = ConcatDataset([train_set, pseudo_set])\n            train_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n\n        # ---------- Training ----------\n        # Make sure the model is in train mode before training.\n        model.train()\n\n        # These are used to record information in training.\n        train_loss = []\n        train_accs = []\n\n        # Iterate the training set by batches.\n        for batch in tqdm(train_loader):\n\n            # A batch consists of image data and corresponding labels.\n            imgs, labels = batch\n\n            # Forward the data. (Make sure data and model are on the same device.)\n            logits = model(imgs.to(device))\n\n            # Calculate the cross-entropy loss.\n            # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n            loss = criterion(logits, labels.to(device))\n\n            # Gradients stored in the parameters in the previous step should be cleared out first.\n            optimizer.zero_grad()\n\n            # Compute the gradients for parameters.\n            loss.backward()\n\n            # Clip the gradient norms for stable training.\n            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n\n            # Update the parameters with computed gradients.\n            optimizer.step()\n\n            # Compute the accuracy for current batch.\n            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n            # Record the loss and accuracy.\n            train_loss.append(loss.item())\n            train_accs.append(acc)\n        # The average loss and accuracy of the training set is the average of the recorded values.\n        train_loss = sum(train_loss) / len(train_loss)\n        train_acc = sum(train_accs) / len(train_accs)\n\n        # Print the information.\n        print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n        # ---------- Validation ----------\n        # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n        model.eval()\n\n        # These are used to record information in validation.\n        valid_loss = []\n        valid_accs = []\n\n        # Iterate the validation set by batches.\n        for batch in tqdm(valid_loader):\n\n            # A batch consists of image data and corresponding labels.\n            imgs, labels = batch\n\n            # We don't need gradient in validation.\n            # Using torch.no_grad() accelerates the forward process.\n            with torch.no_grad():\n                logits = model(imgs.to(device))\n\n            # We can still compute the loss (but not the gradient).\n            loss = criterion(logits, labels.to(device))\n\n            # Compute the accuracy for current batch.\n            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n            # Record the loss and accuracy.\n            valid_loss.append(loss.item())\n            valid_accs.append(acc)\n        # The average loss and accuracy for entire validation set is the average of the recorded values.\n        valid_loss = sum(valid_loss) / len(valid_loss)\n        valid_acc = sum(valid_accs) / len(valid_accs)\n\n        # Print the information.\n        print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(valid_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(valid_acc) \n\n    # apply a Savitzky-Golay filter to smooth the curve\n    train_loss_smooth = savgol_filter(history['train_loss'], window_length=5, polyorder=1)\n    val_loss_smooth = savgol_filter(history['val_loss'], window_length=5, polyorder=1)\n    plt.plot([*range(1, n_epochs+1)], train_loss_smooth)\n    plt.plot([*range(1, n_epochs+1)], val_loss_smooth)\n    plt.xlabel(\"Epoches\")\n    plt.ylabel(\"Loss\")\n    plt.savefig(\"./train-val loss.jpg\")\n\n\n    return model,history,train_loss,valid_loss,train_acc,valid_acc\n    \n    \n    \n# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\n# Initialize a model, and put it on the device specified.\nmodel = Classifier().to(device)\n#model = torchvision.models.resnet50(pretrained=False).to(device)\nmodel.device = device\n\n# For the classification task, we use cross-entropy as the measurement of performance.\ncriterion = nn.CrossEntropyLoss()\n\n# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n\n# The number of training epochs.\nn_epochs = 100\n# Whether to do semi-supervised learning.\ndo_semi = False\nbatch_size = 256\nk=5\nsplits=KFold(n_splits=k,shuffle=True) \nconcat_dataset = ConcatDataset([train_set, train_set])\n\nfor fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(concat_dataset)))):\n    print('Fold {}'.format(fold + 1))\n    \n    train_sampler = SubsetRandomSampler(train_idx)\n    valid_sampler = SubsetRandomSampler(val_idx)\n    train_loader = DataLoader(concat_dataset, batch_size=batch_size,  num_workers=0, sampler=train_sampler, pin_memory=True)\n    valid_loader = DataLoader(concat_dataset, batch_size=batch_size, num_workers=0,sampler=valid_sampler, pin_memory=True)\n    model,history, train_loss,valid_loss,train_acc,valid_acc = Training(model, device, criterion, optimizer, n_epochs, do_semi, train_loader,test_loader,unlabeled_set)\n    break\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-09T03:51:56.232125Z","iopub.execute_input":"2023-03-09T03:51:56.232760Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cpu\nFold 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe208054c5884522bcf316fb49ad35c6"}},"metadata":{}},{"name":"stdout","text":"[ Train | 001/100 ] loss = 2.41584, acc = 0.10563\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf559445c9cc475fb706e87396e08c42"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 001/100 ] loss = 2.39945, acc = 0.09556\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a72b6682e964faeaf711b76c52dca19"}},"metadata":{}},{"name":"stdout","text":"[ Train | 002/100 ] loss = 2.31950, acc = 0.15757\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22fb89b9b2d54d288a4081e1b4eeed06"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 002/100 ] loss = 2.35739, acc = 0.14006\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bc109d216e84b0c8cc0dc8e1e2e13a7"}},"metadata":{}},{"name":"stdout","text":"[ Train | 003/100 ] loss = 2.25587, acc = 0.19587\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b673c243f7940bca70d593a31492ac7"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 003/100 ] loss = 2.31892, acc = 0.15848\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0a591d4dbf4e009084fa89e9d6537f"}},"metadata":{}},{"name":"stdout","text":"[ Train | 004/100 ] loss = 2.21266, acc = 0.21257\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7629efbdde5400b94d9dc29d0ec3a8c"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 004/100 ] loss = 2.18217, acc = 0.23297\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d68878020714c709edf056abc5f88c7"}},"metadata":{}},{"name":"stdout","text":"[ Train | 005/100 ] loss = 2.16044, acc = 0.23299\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97d1a7dd331f42e8ac42176072ad44c6"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 005/100 ] loss = 2.16961, acc = 0.23364\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3f1de7779a24a7d820ad6e6a2612939"}},"metadata":{}},{"name":"stdout","text":"[ Train | 006/100 ] loss = 2.09588, acc = 0.25142\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe6e19dbce704eba9b5b5031f7e77d0f"}},"metadata":{}}]},{"cell_type":"code","source":"# Make sure the model is in eval mode.\n# Some modules like Dropout or BatchNorm affect if the model is in training mode.\nmodel.eval()\n\n# Initialize a list to store the predictions.\npredictions = []\n\n# Iterate the testing set by batches.\nfor batch in tqdm(test_loader):\n    # A batch consists of image data and corresponding labels.\n    # But here the variable \"labels\" is useless since we do not have the ground-truth.\n    # If printing out the labels, you will find that it is always 0.\n    # This is because the wrapper (DatasetFolder) returns images and labels for each batch,\n    # so we have to create fake labels to make it work normally.\n    imgs, labels = batch\n\n    # We don't need gradient in testing, and we don't even have labels to compute loss.\n    # Using torch.no_grad() accelerates the forward process.\n    with torch.no_grad():\n        logits = model(imgs.to(device))\n\n    # Take the class with greatest logit as prediction and record it.\n    predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save predictions into the file.\nwith open(\"predict.csv\", \"w\") as f:\n\n    # The first row must be \"Id, Category\"\n    f.write(\"Id,Category\\n\")\n\n    # For the rest of the rows, each image id corresponds to a predicted class.\n    for i, pred in  enumerate(predictions):\n         f.write(f\"{i},{pred}\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}